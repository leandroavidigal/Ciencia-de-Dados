{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37de8da3-f4ee-4d5d-bb28-c2a40619c356",
   "metadata": {},
   "source": [
    "# Hiperparâmetros do Random Forest (RF) e Seus Propósitos  \n",
    "\n",
    "O **Random Forest (RF)** é um algoritmo de aprendizado de máquina baseado em um conjunto de árvores de decisão, amplamente utilizado para **classificação** e **regressão**. Ele combina diversas árvores de decisão, reduzindo a variância dos modelos individuais e melhorando a generalização dos resultados. Para otimizar sua performance, o RF possui diversos **hiperparâmetros** que influenciam sua capacidade preditiva e eficiência computacional.\n",
    "\n",
    "Abaixo, detalhamos os principais hiperparâmetros e suas funções, acompanhados de **expressões matemáticas e estatísticas** que fundamentam suas propriedades.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Número de Árvores no Ensemble**  \n",
    "\n",
    "- **`n_estimators`**: Determina o número de árvores $T$ que compõem a floresta.  \n",
    "  - Maior número de árvores reduz a variância e melhora a estabilidade do modelo.\n",
    "  - A previsão final é obtida pela **média das predições** no caso de regressão ou pela **maioria dos votos** no caso de classificação.\n",
    "\n",
    "Para **regressão**, a predição final é:\n",
    "\n",
    "$$ \\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} f_t(x) $$\n",
    "\n",
    "Para **classificação**, a predição é baseada na **moda das predições** individuais:\n",
    "\n",
    "$$ \\hat{y} = \\arg\\max_{c} \\sum_{t=1}^{T} \\mathbb{1} (f_t(x) = c) $$\n",
    "\n",
    "onde $f_t(x)$ representa a previsão da $t$-ésima árvore e $\\mathbb{1}$ é a função indicadora.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Critério de Qualidade da Divisão**  \n",
    "\n",
    "- **`criterion`**: Define a métrica utilizada para avaliar a qualidade da divisão dos nós.  \n",
    "\n",
    "  - Para **classificação**, os critérios mais comuns são:  \n",
    "    - **Índice de Gini** $G$: Mede a impureza dos nós. Quanto menor o Gini, melhor a separação das classes.\n",
    "\n",
    "      $$ G = 1 - \\sum_{i=1}^{C} p_i^2 $$\n",
    "\n",
    "      onde $p_i$ é a proporção de amostras da classe $i$ no nó e $C$ é o número total de classes.\n",
    "\n",
    "    - **Entropia** $H$: Mede o grau de desordem do nó.\n",
    "\n",
    "      $$ H = - \\sum_{i=1}^{C} p_i \\log_2(p_i) $$\n",
    "\n",
    "  - Para **regressão**, os critérios incluem:\n",
    "    - **Erro Quadrático Médio (MSE)**:\n",
    "\n",
    "      $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "    - **Erro Absoluto Médio (MAE)**:\n",
    "\n",
    "      $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Controle da Complexidade das Árvores**  \n",
    "\n",
    "- **`max_depth`**: Controla a **profundidade máxima** de cada árvore.  \n",
    "  - Árvores mais profundas aprendem padrões complexos, mas podem superajustar os dados (**overfitting**).\n",
    "\n",
    "- **`min_samples_split`**: Número mínimo de amostras exigido para dividir um nó.  \n",
    "  - Um valor maior reduz a granularidade das divisões e melhora a **generalização**.\n",
    "\n",
    "- **`min_samples_leaf`**: Define o número mínimo de amostras que um nó folha deve conter.  \n",
    "  - Evita que folhas tenham poucas amostras, reduzindo **overfitting**.\n",
    "\n",
    "- **`max_features`**: Define o número de variáveis consideradas para cada divisão em uma árvore.  \n",
    "  - Para **classificação**, o padrão é:\n",
    "\n",
    "    $$ max\\_features = \\sqrt{p} $$\n",
    "\n",
    "  - Para **regressão**, o padrão é:\n",
    "\n",
    "    $$ max\\_features = \\frac{p}{3} $$\n",
    "\n",
    "  onde $p$ é o número total de variáveis preditoras.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Estratégia de Amostragem**  \n",
    "\n",
    "- **`bootstrap`**: Define se a técnica de amostragem com reposição será usada para criar subconjuntos bootstrap.  \n",
    "  - Quando ativado, cada árvore é treinada em um subconjunto de dados amostrado aleatoriamente.\n",
    "\n",
    "- **`oob_score`**: Permite a **validação out-of-bag (OOB)**, utilizando amostras não selecionadas pelo Bootstrap para estimar o desempenho do modelo.\n",
    "\n",
    "- **`max_samples`**: Controla a fração das amostras usadas em cada árvore, influenciando o viés e a variância do modelo.\n",
    "\n",
    "A **probabilidade de uma amostra ser excluída do Bootstrap** é aproximadamente:\n",
    "\n",
    "$$ P_{exclusão} = \\left( 1 - \\frac{1}{n} \\right)^n \\approx \\frac{1}{e} \\approx 0.368 $$\n",
    "\n",
    "o que significa que **cerca de 36,8% das amostras não são usadas** em cada árvore, sendo aproveitadas para a validação OOB.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Otimização do Treinamento**  \n",
    "\n",
    "- **`n_jobs`**: Controla quantos núcleos de CPU serão usados no treinamento.  \n",
    "  - `-1` utiliza todos os núcleos disponíveis para **paralelizar o treinamento**.\n",
    "\n",
    "- **`random_state`**: Define um **seed** para a aleatoriedade do modelo, garantindo reprodutibilidade.\n",
    "\n",
    "- **`warm_start`**: Permite reutilizar um modelo já treinado e adicionar novas árvores.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Ajuste de Classes e Poda das Árvores**  \n",
    "\n",
    "- **`class_weight`**: Ajusta os pesos das classes para corrigir desbalanceamentos.  \n",
    "  - `\"balanced\"` ajusta automaticamente os pesos para compensar classes minoritárias.\n",
    "\n",
    "- **`ccp_alpha`**: Define um **parâmetro de poda baseado em complexidade**.  \n",
    "  - Árvores são podadas removendo divisões que resultam em ganho de impureza menor que $ccp\\_alpha$.\n",
    "\n",
    "  A função de custo utilizada para poda é:\n",
    "\n",
    "  $$ R(T) = R(T_t) + \\alpha |T| $$\n",
    "\n",
    "  onde $R(T)$ é o erro da árvore completa, $R(T_t)$ é o erro da subárvore podada e $|T|$ é o número de nós folha.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusão**  \n",
    "\n",
    "O **Random Forest** oferece uma abordagem robusta para modelagem preditiva, combinando árvores de decisão de maneira eficiente para **reduzir a variância** e **aumentar a precisão**. A correta configuração de seus **hiperparâmetros** permite encontrar um equilíbrio ideal entre **viés** e **variância**, garantindo um modelo altamente generalizável.\n",
    "\n",
    "### **Diretrizes para Ajuste Eficiente**  \n",
    "- **Para reduzir overfitting**, regularizar `max_depth`, `min_samples_split` e `min_samples_leaf`.  \n",
    "- **Para aumentar a diversidade das árvores**, ajustar `max_features` e `bootstrap=True`.  \n",
    "- **Para acelerar o treinamento**, utilizar `n_jobs=-1` e limitar `n_estimators` para evitar tempos excessivos.  \n",
    "- **Para dados desbalanceados**, considerar `class_weight=\"balanced\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fa398-de8b-4e4a-a322-11e39cd03612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

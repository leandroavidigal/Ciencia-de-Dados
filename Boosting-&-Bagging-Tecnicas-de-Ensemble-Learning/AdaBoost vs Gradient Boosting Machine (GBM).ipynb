{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4b6575-ec87-489c-81f1-b49a29cbba70",
   "metadata": {},
   "source": [
    "# AdaBoost vs Gradient Boosting Machine (GBM): Compara√ß√£o e Otimiza√ß√£o\n",
    "\n",
    "## **1. Diferen√ßas Fundamentais entre AdaBoost e GBM**\n",
    "\n",
    "O **AdaBoost (Adaptive Boosting)** e o **Gradient Boosting Machine (GBM)** s√£o ambos algoritmos de **ensemble learning**, mas possuem diferen√ßas significativas em suas abordagens e funcionamento. A seguir, destacamos cinco diferen√ßas principais:\n",
    "\n",
    "### üîπ **M√©todo de Ajuste dos Erros**\n",
    "**AdaBoost**: Ajusta os pesos das observa√ß√µes de acordo com os erros do modelo anterior. Os exemplos mal classificados recebem pesos maiores, tornando-os mais influentes na pr√≥xima itera√ß√£o.\n",
    "  \n",
    "  $$ \\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1 - e_t}{e_t} \\right) $$\n",
    "\n",
    "  onde:\n",
    "  - $ e_t $ √© a taxa de erro da √°rvore na itera√ß√£o $ t $.\n",
    "  - $ \\alpha_t $ √© o peso atribu√≠do √† √°rvore no ensemble.\n",
    "\n",
    "**GBM**: Em vez de ajustar pesos, ele minimiza a **fun√ß√£o de perda** ajustando cada novo modelo para prever o **gradiente do erro** do modelo anterior.\n",
    "\n",
    "  $$ F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x) $$\n",
    "\n",
    "  onde:\n",
    "  - $ F_m(x) $ √© o modelo atualizado na itera√ß√£o $ m $.\n",
    "  - $ h_m(x) $ √© a nova √°rvore ajustada ao gradiente do erro.\n",
    "  - $ \\gamma_m $ √© a taxa de aprendizado que controla a contribui√ß√£o da nova √°rvore.\n",
    "\n",
    "### üîπ **Flexibilidade na Fun√ß√£o de Perda**\n",
    "**AdaBoost**: Usa **fun√ß√£o de perda exponencial** por padr√£o:\n",
    "\n",
    "  $$ L(y, F(x)) = e^{-yF(x)} $$\n",
    "\n",
    "**GBM**: Permite a escolha de diversas fun√ß√µes de perda, incluindo:\n",
    "  - **Erro quadr√°tico** para regress√£o: $ L(y, \\hat{y}) = (y - \\hat{y})^2 $\n",
    "  - **Log-loss** para classifica√ß√£o bin√°ria: $ L(y, \\hat{y}) = - y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y}) $\n",
    "\n",
    "### üîπ **Sensibilidade a Outliers**\n",
    "**AdaBoost**: Pode ser mais **sens√≠vel a outliers**, pois os exemplos dif√≠ceis de classificar recebem mais peso e podem influenciar excessivamente o modelo.\n",
    "\n",
    "**GBM**: Possui mais **flexibilidade na fun√ß√£o de perda**, permitindo que se adapte melhor a dados ruidosos.\n",
    "\n",
    "### üîπ **Complexidade Computacional**\n",
    "**AdaBoost**: Normalmente **mais r√°pido**, pois apenas ajusta os pesos das observa√ß√µes a cada itera√ß√£o.\n",
    "\n",
    "**GBM**: **Mais lento**, pois precisa calcular o **gradiente do erro** e ajustar a √°rvore para minimiz√°-lo.\n",
    "\n",
    "### üîπ **Complexidade dos Modelos Base**\n",
    "**AdaBoost**: Utiliza **stumps** (√°rvores rasas com profundidade 1) como base learners.\n",
    "\n",
    "**GBM**: Pode usar √°rvores **mais profundas**, o que permite capturar rela√ß√µes mais complexas.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Implementa√ß√£o do GBM no Scikit-Learn**\n",
    "Abaixo est√£o os exemplos de **classifica√ß√£o** e **regress√£o** utilizando o **Gradient Boosting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c18906a-757b-4c3b-8c96-d810bcafbd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41a352d-e340-44bd-b6bf-431f9ac42aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, \n",
    "                                 learning_rate=1.0, \n",
    "                                 max_depth=1, \n",
    "                                 random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb37829-7bbe-4fff-89d6-68005638a2f9",
   "metadata": {},
   "source": [
    "### Exemplo 2: Regress√£o com GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39199baf-62e1-493b-a692-0fb21529054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a068a940-b305-4c6a-bccb-cef99e59d0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, \n",
    "                                learning_rate=0.1, \n",
    "                                max_depth=1, \n",
    "                                random_state=0, \n",
    "                                loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86622a0-b73c-4c06-84dd-76f20ddf2fe9",
   "metadata": {},
   "source": [
    "## **3. Principais Hiperpar√¢metros do GBM**\n",
    "A escolha dos hiperpar√¢metros √© fundamental para o desempenho do modelo GBM. Aqui est√£o os mais importantes:\n",
    "\n",
    "#### **`n_estimators` (N√∫mero de √Årvores)**\n",
    "- Define o **n√∫mero de √°rvores** adicionadas ao ensemble.\n",
    "- Quanto maior, mais preciso pode ser o modelo, mas aumenta o risco de **overfitting**.\n",
    "\n",
    "#### **`learning_rate` (Taxa de Aprendizado)**\n",
    "- Controla a **contribui√ß√£o de cada √°rvore** no modelo final:\n",
    "\n",
    "  $$ F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x) $$\n",
    "\n",
    "- Taxas menores precisam de mais √°rvores para obter um bom desempenho.\n",
    "\n",
    "#### **`max_depth` (Profundidade M√°xima das √Årvores)**\n",
    "- Define a **complexidade das √°rvores**.\n",
    "- **Valores altos** permitem capturar padr√µes complexos, mas podem causar **overfitting**.\n",
    "\n",
    "#### **`max_features` (N√∫mero M√°ximo de Vari√°veis por Divis√£o)**\n",
    "- Controla **quantas vari√°veis** s√£o consideradas ao dividir cada n√≥.\n",
    "\n",
    "#### **`warm_start` (Incrementa√ß√£o do Modelo)**\n",
    "- Permite adicionar mais √°rvores ao modelo sem perder as anteriores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d6267-19e7-47d2-b795-a68bcd361515",
   "metadata": {},
   "source": [
    "## **4. Otimiza√ß√£o de Hiperpar√¢metros com GridSearch**\n",
    "Podemos usar **GridSearch** para encontrar os **melhores hiperpar√¢metros**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ac016b-f9cb-4e7c-a2b4-12b164f21e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d473856d-bb0c-495e-a17d-2632eb805035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 783 ms, total: 1min 25s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_squared_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>3.625209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1</td>\n",
       "      <td>3.667692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.733205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.733272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>3.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1</td>\n",
       "      <td>21.909960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>9</td>\n",
       "      <td>22.400111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6</td>\n",
       "      <td>22.472923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>22.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>24.252178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  learning_rate  max_depth  mean_squared_error\n",
       "0           1000           0.03          1            3.625209\n",
       "1           1000           0.06          1            3.667692\n",
       "2          10000           0.10          3            3.733205\n",
       "3           1000           0.10          3            3.733272\n",
       "4            100           0.10          3            3.774987\n",
       "..           ...            ...        ...                 ...\n",
       "59            10           0.03          1           21.909960\n",
       "60            10           0.01          9           22.400111\n",
       "61            10           0.01          6           22.472923\n",
       "62            10           0.01          3           22.902975\n",
       "63            10           0.01          1           24.252178\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "estimators     = [10, 100, 1000, 10000]\n",
    "learning_rates = [0.01, 0.03, 0.06, 0.1]\n",
    "max_depths     = [1, 3, 6, 9]\n",
    "\n",
    "grid_search = []\n",
    "\n",
    "for n in estimators:\n",
    "    for rate in learning_rates:\n",
    "        for depth in max_depths:\n",
    "            est = GradientBoostingRegressor(n_estimators=n, \n",
    "                                            learning_rate=rate, \n",
    "                                            max_depth=depth, \n",
    "                                            random_state=0, \n",
    "                                            loss='squared_error').fit(X_train, y_train)\n",
    "            grid_search.append([n, rate, depth, mean_squared_error(y_test, est.predict(X_test))])\n",
    "            \n",
    "(pd.DataFrame(data=grid_search, \n",
    "              columns=['n_estimators', 'learning_rate', 'max_depth', 'mean_squared_error'])\n",
    "   .sort_values(by='mean_squared_error', \n",
    "                ascending=True, \n",
    "                ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fde36-2bb7-46f9-ba26-55627bea0d8a",
   "metadata": {},
   "source": [
    "## **5. Diferen√ßa entre GBM e Stochastic GBM**\n",
    "O **Stochastic GBM** introduz **aleatoriedade** na sele√ß√£o de amostras para cada √°rvore, reduzindo o **overfitting**.\n",
    "\n",
    "- **GBM tradicional**: Usa **todo o conjunto de dados** em cada itera√ß√£o.\n",
    "- **Stochastic GBM**: Usa **subconjuntos aleat√≥rios**, combinando **Bagging e Boosting**.\n",
    "\n",
    "Isso melhora a **generaliza√ß√£o do modelo** e reduz o tempo de treinamento.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclus√£o**\n",
    "‚úÖ **AdaBoost** √© mais simples e r√°pido, mas sens√≠vel a outliers.  \n",
    "‚úÖ **GBM** √© mais flex√≠vel, usa gradientes e suporta v√°rias fun√ß√µes de perda.  \n",
    "‚úÖ **O ajuste fino dos hiperpar√¢metros √© essencial** para maximizar o desempenho do GBM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1ad62-010a-471b-bb84-50c47e8362d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
